{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:49:02.747994Z",
     "start_time": "2023-10-15T05:48:56.786859Z"
    }
   },
   "id": "2e7c4dbf55512178"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data extraction\n",
    "extract current data from the database and prepare for processing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3acda61db2d57078"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# TODO: extract data and target\n",
    "\n",
    "X = []  # TODO: choose scaler\n",
    "Y = []"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:49:02.756464Z",
     "start_time": "2023-10-15T05:49:02.752668Z"
    }
   },
   "id": "97661c6a6806efa7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Data preparation\n",
    "Split between training and testing set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c044c0884df392e1"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtorch\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TensorDataset, DataLoader\n\u001B[0;32m----> 3\u001B[0m train_x, test_x, train_y, test_y \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mY\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m test_x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(test_x)\n\u001B[1;32m      6\u001B[0m test_y \u001B[38;5;241m=\u001B[39m test_y  \u001B[38;5;66;03m# TODO: choose encoding (one_hot, etc.)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/nextgenbike/venv/lib/python3.9/site-packages/sklearn/utils/_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    209\u001B[0m         )\n\u001B[1;32m    210\u001B[0m     ):\n\u001B[0;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    221\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/nextgenbike/venv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2649\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2646\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[1;32m   2648\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m-> 2649\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_shuffle_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2650\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_test_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\n\u001B[1;32m   2651\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2653\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m   2654\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stratify \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/nextgenbike/venv/lib/python3.9/site-packages/sklearn/model_selection/_split.py:2305\u001B[0m, in \u001B[0;36m_validate_shuffle_split\u001B[0;34m(n_samples, test_size, train_size, default_test_size)\u001B[0m\n\u001B[1;32m   2302\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(n_train), \u001B[38;5;28mint\u001B[39m(n_test)\n\u001B[1;32m   2304\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2305\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2306\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWith n_samples=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, test_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and train_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2307\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresulting train set will be empty. Adjust any of the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2308\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maforementioned parameters.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_samples, test_size, train_size)\n\u001B[1;32m   2309\u001B[0m     )\n\u001B[1;32m   2311\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m n_train, n_test\n",
      "\u001B[0;31mValueError\u001B[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
    "\n",
    "test_x = torch.Tensor(test_x)\n",
    "test_y = test_y  # TODO: choose encoding (one_hot, etc.)\n",
    "\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset)\n",
    "test_loader = DataLoader(test_dataset)\n",
    "\n",
    "classes = ['asphalt', 'gravel', 'nature']  # TODO: define all classes"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:49:02.987659Z",
     "start_time": "2023-10-15T05:49:02.758240Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Building the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dfaad9d2cceee00a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class SurfaceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SurfaceModel, self).__init__()\n",
    "        # Todo: Define\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Todo: Apply\n",
    "        return x\n",
    "\n",
    "\n",
    "model = SurfaceModel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-15T05:49:02.987186Z"
    }
   },
   "id": "988c493582fa50e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the Model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb319204e1a5117d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "momentum = 0.9\n",
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")  # for testing on my Mac\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# criterion = nn.MSELoss()\n",
    "# criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "print(\"Training STARTED\")\n",
    "model.to(device)\n",
    "for e in range(0, num_epochs):\n",
    "    model.train()  # set the model in training mode\n",
    "    totalTrainLoss = 0  # initialize the total training and validation loss\n",
    "    totalValLoss = 0\n",
    "    # initialize the number of correct predictions in the training and validation step\n",
    "    trainCorrect = 0\n",
    "    valCorrect = 0\n",
    "\n",
    "    for i, (x, y) in enumerate(train_loader):  # loop over the training set\n",
    "        (x, y) = (x.to(device), y.to(torch.float32).to(device))  # send the input to the device\n",
    "        pred = model(x)  # perform a forward pass and calculate the training loss\n",
    "\n",
    "        loss = criterion(pred, y)\n",
    "        # loss = criterion(m(pred), y) # for NLLLoss()\n",
    "        \n",
    "        # zero out the gradients, perform the backpropagation step, and update the weights\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # add the loss to the total training loss so far and calculate the number of correct predictions\n",
    "        totalTrainLoss += loss\n",
    "\n",
    "    if (e + 1) % 10 == 0 | e == 0:\n",
    "        print(\"Epoch\", e, \"Training Loss:\", totalTrainLoss.item())\n",
    "\n",
    "print(\"Training FINISHED\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:49:02.991147Z",
     "start_time": "2023-10-15T05:49:02.988690Z"
    }
   },
   "id": "e7b124385ff76145"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "530667021e47b60d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import util"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-15T05:49:02.990684Z"
    }
   },
   "id": "44ea3b85f8392ef1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f994401f1780a325"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "training_loss, class_correct, class_total = util.compute_accuracy(model, train_loader, device, criterion)\n",
    "\n",
    "# average training loss\n",
    "training_loss = training_loss/len(train_loader.dataset)\n",
    "print('Training Loss: {:.6f}\\n'.format(training_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Training Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100.0 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Training Accuracy of %5s: N/A ' % (classes[i]))\n",
    "\n",
    "print('\\Training Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-15T05:49:02.993859Z",
     "start_time": "2023-10-15T05:49:02.993498Z"
    }
   },
   "id": "62ce83cb80d2b080"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testing Results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8f6eb8ac44f60ba2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_loss, class_correct, class_total = util.compute_accuracy(model, test_loader, device, criterion)\n",
    "\n",
    "# average test loss\n",
    "test_loss = test_loss/len(test_loader.dataset)\n",
    "print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "for i in range(10):\n",
    "    if class_total[i] > 0:\n",
    "        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (\n",
    "            classes[i], 100.0 * class_correct[i] / class_total[i],\n",
    "            np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "    else:\n",
    "        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))\n",
    "\n",
    "print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (\n",
    "    100. * np.sum(class_correct) / np.sum(class_total),\n",
    "    np.sum(class_correct), np.sum(class_total)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-10-15T05:49:02.994435Z"
    }
   },
   "id": "d71d827209b64b7c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
